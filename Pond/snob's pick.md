# snob's pick 
Í∞úÏù∏Ï†ÅÏúºÎ°ú Ï§ëÏöîÌïòÎã§Í≥† ÏÉùÍ∞ÅÎêòÎäî LLM Í¥ÄÎ†® ÎÖºÎ¨∏Îì§ÏùÑ Ï†ú ÎßòÎåÄÎ°ú Î™®ÏïÑ Î≥¥ÏïòÏäµÎãàÎã§ü§ó

## LLM
<div align="center">
  
| Paper | a.k.a | Affiliation | Published date | # | Desc.|
|-------|-------|-------------|----------------|---|------|
[RoFormer: Enhanced Transformer with Rotary Position Embedding](https://arxiv.org/abs/2104.09864) | RoPE | Zhuiyi Technology | August. 2022 | \#PE<br>\#RPE<br>\#ComplexPlane
[TRAIN SHORT, TEST LONG:<br>ATTENTION WITH LINEAR BIASES<br>ENABLES INPUT LENGTH EXTRAPOLATION](https://arxiv.org/abs/2108.12409)| ALiBi¬†| Facebook | April. 2022 | \#seq_len<br>\#Extrapolation<br>\#Efficient
[A Length-Extrapolatable Transformer](https://arxiv.org/abs/2212.10554) | xPos | Microsoft | December. 2022 | \#PE<br>\#RoPE<br>\#ComplexPlane
[\*Extending Context is Hard‚Ä¶but not Impossible](https://kaiokendev.github.io/context) | kaiokendev | | February. 2023 |
[EXTENDING CONTEXT WINDOW OF LARGE LANGUAGE MODELS VIA POSITION INTERPOLATION](https://arxiv.org/abs/2306.15595) | post-kaiokendev | Meta | Jun. 2023 | \#seq_len<br>\#Interpolation<br>\#RoPE
[RWKV: <br>Reinventing RNNs for the Transformer Era](https://arxiv.org/abs/2305.13048) | RWKV | RWKV Foundation | May. 2023 | \#Architecture<br>\#Recurrent<br>\#Efficient
[Retentive Network: A Successor to Transformer for Large Language Models](https://arxiv.org/abs/2307.08621) | RetNet | Microsoft | July. 2023 | \#Architecture<br>\#Recurrent<br>\#Efficient
[Hyena Hierarchy: Towards Larger Convolutional Language Models](https://arxiv.org/abs/2302.10866) | Hyena |  | April. 2023 | \#Architecture<br>\#Recurrent<br>\#Efficient
[Training Compute-Optimal Large Language Models](https://arxiv.org/abs/2203.15556) | Chinchilla | | May. 2022 | \#Model<br>\#Foundation<br>\#Methodology
[LLaMA: Open and Efficient Foundation Language Models](https://arxiv.org/abs/2302.13971) | LLaMA | Meta | February. 2023 | \#Model<br>\#Foundation<br>
[\*Alpaca: A Strong, Replicable Instruction-Following Model](https://crfm.stanford.edu/2023/03/13/alpaca.html) | Alpaca | Stanford University | March. 2023 | \#Model<br>\#Finetuning<br>\#self-instruct
[LIMA: Less Is More for Alignment](https://arxiv.org/abs/2305.11206) | LIMA | Meta | May. 2023 | \#Model<br>\#Finetuning<br>\#Data-centric
[Orca: Progressive Learning from Complex Explanation Traces of GPT-4](https://arxiv.org/abs/2306.02707) | Orca | Microsoft | June. 2023 | \#Model<br>\#Finetuning<br>\#Methodology
[Platypus: Quick, Cheap, and Powerful Refinement of LLMs](https://platypus-llm.github.io/Platypus.pdf) | Platypus | Boston University | August. 2023 | \#Model<br>\#Finetuning<br>\#Methodology
[LoRA: Low-Rank Adaptation of Large Language Models](https://arxiv.org/abs/2205.05638) | LoRA | NC univ. \@chapel hill  | October. 2021 | \#PEFT
[QLoRA: Efficient Finetuning of Quantized LLMs](https://arxiv.org/abs/2305.14314) | QLoRA | University of Washington  | May. 2023 | \#PEFT<br>\#LoRA
[Few-Shot Parameter-Efficient Fine-Tuning is Better and Cheaper than In-Context Learning](https://arxiv.org/abs/2205.05638) | IA3 | NC univ. \@chapel hill | August. 2022 | \#PEFT<br>\#FewShot-learning
[Stack More Layers Differently: High-Rank Training Through Low-Rank Updates](https://arxiv.org/abs/2307.05695) | ReLoRA | Massachusetts Lowel | August. 2023 | \#PEFT<br>\#LoRA
[Knowledge Distillation of Large Language Models](https://arxiv.org/pdf/2306.08543.pdf) | MiniLLM | CoAI Group | Jun. 2023 | \#Distillation<br>\#White-box KD
[LoraHub: Efficient Cross-Task Generalization via Dynamic LoRA Composition](https://arxiv.org/abs/2307.13269) | LoRAHub | Sea AI Lab | Jun. 2023 | \#PEFT<br>\#LoRA<br>\#Compose

</div>

\*: not a paper
